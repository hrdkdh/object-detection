{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch 모델 파일을 tensorflow 파일로 변환, 이미지 입력 후 prediction하고, 후처리하여 출력"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드를 실행시켜 학습이 완료된 pytorch 모델파일(best.pt)을 tensorflow 모델파일(saved model)로 변환합니다.\n",
    "\n",
    ">{가상환경 경로명}\\Scripts\\python.exe {yolov5-master 경로명}\\export.py --weights {yolov5-master 경로명}\\runs\\train\\exp\\weights\\best.pt --include saved_model\n",
    "\n",
    "{yolov5-master 경로명}\\runs\\train\\exp\\weights 폴더로 들어가면 tensorflow 모델로 변환된 'saved_model' 폴더가 생성된 것을 확인할 수 있습니다.\n",
    "\n",
    "'saved_model' 폴더를 실습파일이 있는 경로(현재 보고 있는 이 주피터 노트북 파일이 있는 경로)로 복사해 옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf saved_model로 변환된 모델 파일 로드\n",
    "model = tf.saved_model.load(\"best_saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델의 입출력 구조 확인 → 입력은 (1, 640, 640, 3), 출력은 (1, 25200, 7)임을 알 수 있음\n",
    "for k, v in model.signatures.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 하나를 로드해 모델에 입력으로 넣고 prediction한 결과를 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#테스트할 이미지를 행렬로 로드\n",
    "src = cv2.imread(\"test_images/test_image3.png\")\n",
    "src_converted = cv2.cvtColor(src, cv2.COLOR_BGR2RGB)\n",
    "src_converted.shape # -> (2268, 4032, 3) 4032 x 2268 사이즈의 3채널 컬러이미지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#우리가 만든 모델은 (1, 640, 640, 3)을 입력으로 받으므로, 그에 맞게 resize & reshape을 진행\n",
    "# ※입력에 차원이 하나 더 붙어있는 이유는 배치 때문으로, resize 후 reshape을 통해 변환\n",
    "\n",
    "src_resized = cv2.resize(src_converted, (640, 640))\n",
    "src_resized.shape # -> (640, 640, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_resized = src_resized/255. #픽셀값이 0~1 사이 값으로 rescaling되어 트레이닝 되었음. 추론도 0~1 사이의 값으로 변환하고 시도해야 함\n",
    "src_resized = src_resized.reshape((1, 640, 640, 3))\n",
    "src_resized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#추론 실행\n",
    "pred = model(src_resized)\n",
    "pred[0].shape # -> TensorShape([1, 25200, 7])  *numpy가 아니라 tf 프레임웍 포맷에 맞춰 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#제일 앞에 있는 1차원은 의미 없고 나머지 차원이 중요 -> 25200행 * 7열\n",
    "#25200행 : 입력에 따라 추론한 25200개의 객체검출 결과\n",
    "#7열 : 행별 결과내용 → [cx, cy, w, h, obj conf, c1 conf, c2 conf]\n",
    "# 0~3번째 열은 검출한 바운딩 박스 정보\n",
    "# 4번째 열은 바운딩 박스 안에 객체가 있을 확률\n",
    "# 5번째 열은 검출한 객체가 1번 클래스(도로표지판)일 확률\n",
    "# 6번째 열은 검출한 객체가 2번 클래스(신호등)일 확률\n",
    "\n",
    "pred = pred[0][0].numpy() #필요없는 차원들은 제거하고 (25200, 7)만 선택해 numpy 행렬로 변환한 후 pred에 덮어 씌움\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred의 4번째 행(pred[:, 4])를 기준으로 정렬하면 객체가 있을 확률이 높은 순서대로 결과를 볼 수 있음\n",
    "rank = np.argsort(-pred[:, 4])\n",
    "pred = pred[rank]\n",
    "pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추론한 바운딩 박스 정보를 원본 이미지에 그린 후 결과를 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getXyxy(cxcywh):\n",
    "    x1 = cxcywh[0] - (cxcywh[2]/2)\n",
    "    y1 = cxcywh[1] - (cxcywh[3]/2)\n",
    "    x2 = x1 + cxcywh[2]\n",
    "    y2 = y1 + cxcywh[3]\n",
    "    return [x1, y1, x2, y2]\n",
    "\n",
    "def getCandidatesByClass(pred, conf_thres=0.8, min_wh=0.001):\n",
    "    class_len = pred[0].shape[0] - 5 #cx, cy, w, h, c 를 뺀 나머지가 클래스의 개수이다.\n",
    "    candidate_list = []\n",
    "    for class_no in range(class_len):\n",
    "        candidate_list.append({\n",
    "            \"class_no\" : class_no,\n",
    "            \"bboxes(xyxy)\" : []\n",
    "        })\n",
    "    for item in pred:\n",
    "        #class conf 표준화\n",
    "        this_class_no = item[5:].argmax() #클래스별 확률값을 담고 있는 요소들만 슬라이싱한 후 최대값이 위치한 곳의 index 번호 가져오기\n",
    "        this_conf = item[4]\n",
    "        if this_conf >= conf_thres:\n",
    "            #bbox의 너비 높이가 일정 수준 이하인 것들은 제거한다.\n",
    "            bbox = getXyxy(item[:4])\n",
    "            if bbox[2] - bbox[0] < min_wh or bbox[3] - bbox[1] < min_wh:\n",
    "                continue\n",
    "            this_result = bbox\n",
    "            this_result.append(this_conf)\n",
    "            candidate_list[this_class_no][\"bboxes(xyxy)\"].append(this_result)\n",
    "    for class_no in range(class_len):\n",
    "        if len(candidate_list[class_no][\"bboxes(xyxy)\"]) > 0:\n",
    "            candidate_list[class_no][\"bboxes(xyxy)\"] = np.array(candidate_list[class_no][\"bboxes(xyxy)\"])\n",
    "            candidate_list[class_no][\"bboxes(xyxy)\"] = candidate_list[class_no][\"bboxes(xyxy)\"][np.argsort(-candidate_list[class_no][\"bboxes(xyxy)\"][:, -1])]\n",
    "    return candidate_list\n",
    "\n",
    "def IoU(box1, box2):\n",
    "    #서로 겹쳐져 있는지 확인하고, 겹쳐져 있지 않다면 0을 출력한다.\n",
    "    if box1[2] < box2[0] or box1[0] > box2[2] or box1[3] < box2[1] or box1[1] > box2[3]:\n",
    "        return 0\n",
    "\n",
    "    # box = (x1, y1, x2, y2)\n",
    "    box1_area = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n",
    "    box2_area = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n",
    "    \n",
    "    # obtain x1, y1, x2, y2 of the intersection\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "\n",
    "    # compute the width and height of the intersection\n",
    "    w = max(0, x2 - x1 + 1)\n",
    "    h = max(0, y2 - y1 + 1)\n",
    "\n",
    "    inter = w * h\n",
    "    iou = inter / (box1_area + box2_area - inter)\n",
    "    return iou\n",
    "\n",
    "def nms(bboxes, iou_thres = 0.45, merge_thres = 0.75, print_iou=False): #겹치는 면적이 iou_thres를 넘으면 하나만 선택한다.\n",
    "    if len(bboxes) == 0:\n",
    "        return bboxes\n",
    "    B = bboxes\n",
    "\n",
    "    #1) 해당 클래스에 대해 최종적으로 남길 bounding box 를 담을 리스트 D를 생성\n",
    "    # B 중에서 class score 가 가장 높은 bounding box 를 선택하고 D에 추가하고 B에서 삭제\n",
    "    argmax_B = B[:, -1].argmax()\n",
    "    T = B[argmax_B] #B의 최대값을 담을 임시 변수 T 생성\n",
    "    B = np.delete(B, argmax_B, axis=0)\n",
    "    D = None #검사가 완료된 최종 D만 담을 리스트\n",
    "\n",
    "    #2) D에 추가된 class score 가 가장 높은 bounding box 와 B에 담긴 bounding box 와의 IOU (Intersection Over Union) 을 계산\n",
    "    while True:\n",
    "        idx_to_delete = []\n",
    "        for idx, B_item in enumerate(B):\n",
    "            # IOU 가 주어진 임계치 iou_thres보다 크다면 B에서 제거\n",
    "            iou = IoU(T[:4], B_item[:4])\n",
    "            # print(T[:4], B_item[:4])\n",
    "            if iou > iou_thres:\n",
    "                idx_to_delete.append(idx)\n",
    "        B = np.delete(B, idx_to_delete, axis=0)\n",
    "        if D is None:\n",
    "            D = T.copy()\n",
    "            D = D.reshape(1, 5)\n",
    "        else:\n",
    "            D = np.vstack([D, T])\n",
    "        # print(\"D : {}\".format(len(D)), \"B : {}\".format(len(B)))\n",
    "\n",
    "        if len(B) > 0:\n",
    "            #3) 2번 과정을 수행하고 B에 남은 bounding box 중 가장 큰 class score 를 가진 bounding box 를 D에 추가하고 B에서 제거한 후 2번 과정을 반복\n",
    "            #B에 bounding box 가 남지 않을 때까지 반복\n",
    "            argmax_B = B[:, -1].argmax()\n",
    "            T = B[argmax_B]\n",
    "            B = np.delete(B, argmax_B, axis=0)\n",
    "        else:\n",
    "            break\n",
    "    return D\n",
    "\n",
    "conf_thres = 0.25\n",
    "min_wh = 0.002\n",
    "\n",
    "candidate_list = getCandidatesByClass(pred, conf_thres, min_wh)\n",
    "\n",
    "class_names = [\"sign\", \"light\"]\n",
    "bbox_colors = [(0, 255, 0), (0, 0, 255)]\n",
    "\n",
    "dst = cv2.resize(src, (int((900*src.shape[1])/src.shape[0]), 900), interpolation=cv2.INTER_LINEAR) #이미지가 너무 크므로 리사이즈한다.\n",
    "for idx, candidate in enumerate(candidate_list):\n",
    "    class_name = class_names[idx]\n",
    "    color = bbox_colors[idx]\n",
    "    bboxes = nms(candidate_list[idx][\"bboxes(xyxy)\"])\n",
    "    for bbox in bboxes:\n",
    "        conf = bbox[-1]\n",
    "\n",
    "        x1 = int(bbox[0] * dst.shape[1])\n",
    "        y1 = int(bbox[1] * dst.shape[0])\n",
    "        x2 = int(bbox[2] * dst.shape[1])\n",
    "        y2 = int(bbox[3] * dst.shape[0])\n",
    "\n",
    "        dst = cv2.rectangle(dst, (x1, y1), (x2, y2), color, 2)\n",
    "        dst = cv2.putText(dst, \"{}:{:.2f}\".format(class_name, conf), (x1, y1-5), cv2.FONT_HERSHEY_DUPLEX, 0.6, color, 1)\n",
    "cv2.imshow(\"dst\", dst)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "카메라에 연결하여 테스트 해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_thres = 0.25\n",
    "min_wh = 0.002\n",
    "class_names = [\"sign\", \"light\"]\n",
    "bbox_colors = [(0, 255, 0), (0, 0, 255)]\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "cap = cv2.VideoCapture(\"test_images/test_video.mp4\")\n",
    "\n",
    "while True:\n",
    "    retval, frame = cap.read()\n",
    "    if retval is False:\n",
    "        break\n",
    "    src = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    src = cv2.resize(src, (640, 640))\n",
    "    src = src/255.\n",
    "    src = src.reshape((1, 640, 640, 3))\n",
    "    pred = model(src)\n",
    "    pred = pred[0][0].numpy()\n",
    "\n",
    "    candidate_list = getCandidatesByClass(pred, conf_thres, min_wh)\n",
    "\n",
    "    dst = frame.copy()\n",
    "    for idx, candidate in enumerate(candidate_list):\n",
    "        class_name = class_names[idx]\n",
    "        color = bbox_colors[idx]\n",
    "        bboxes = nms(candidate_list[idx][\"bboxes(xyxy)\"])\n",
    "        for bbox in bboxes:\n",
    "            conf = bbox[-1]\n",
    "\n",
    "            x1 = int(bbox[0] * dst.shape[1])\n",
    "            y1 = int(bbox[1] * dst.shape[0])\n",
    "            x2 = int(bbox[2] * dst.shape[1])\n",
    "            y2 = int(bbox[3] * dst.shape[0])\n",
    "\n",
    "            dst = cv2.rectangle(dst, (x1, y1), (x2, y2), color, 2)\n",
    "            dst = cv2.putText(dst, \"{}:{:.2f}\".format(class_name, conf), (x1, y1-5), cv2.FONT_HERSHEY_DUPLEX, 0.6, color, 1)\n",
    "    cv2.imshow(\"dst\", dst)\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lessons\n",
    "\n",
    "1. 물체감지는 라벨링 작업에 시간이 많이 걸리므로, 이번 실습은 일반적으로 공개된 데이터를 활용해 진행할 수 밖에 없었다.\n",
    "\n",
    "2. 현업에서 실제 활용할 목적으로 모델을 만들고자 한다면, 수집한 이미지 데이터에 맞는 라벨링 작업이 필수로 요구된다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('venv_common': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00788648533811c0d4af57f0819dead35b430506c7c842538b9d24915bb6fd49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
