{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch 모델 파일을 tensorflow 파일로 변환"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드를 실행시켜 학습이 완료된 pytorch 모델파일(best.pt)을 tensorflow 모델파일(saved model)로 변환합니다.\n",
    "\n",
    ">{가상환경 경로명}\\Scripts\\python.exe {yolov5-master 경로명}\\export.py --weights {yolov5-master 경로명}\\runs\\train\\exp\\weights\\best.pt --include saved_model\n",
    "\n",
    "{yolov5-master 경로명}\\runs\\train\\exp\\weights 폴더로 들어가면 tensorflow 모델로 변환된 'saved_model' 폴더가 생성된 것을 확인할 수 있습니다.\n",
    "\n",
    "'saved_model' 폴더를 실습파일이 있는 경로(현재 보고 있는 이 주피터 노트북 파일이 있는 경로)로 복사해 옵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트 이미지를 입력으로 넣어 prediction하고 바운딩박스를 그려 이미지로 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. tf로 변환된 모델(pb)을 로드해 모델 객체를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_pruned_6862) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    }
   ],
   "source": [
    "#tf saved_model로 변환된 모델 파일 로드\n",
    "model = tf.saved_model.load(\"backup/best_saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serving_default ConcreteFunction signature_wrapper(*, x)\n",
      "  Args:\n",
      "    x: float32 Tensor, shape=(1, 640, 640, 3)\n",
      "  Returns:\n",
      "    {'output_0': <1>}\n",
      "      <1>: float32 Tensor, shape=(1, 25200, 7)\n"
     ]
    }
   ],
   "source": [
    "#모델의 입출력 구조 확인 → 입력은 (1, 640, 640, 3), 출력은 (1, 25200, 7)임을 알 수 있음\n",
    "for k, v in model.signatures.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 이미지 하나를 로드해 모델에 입력으로 넣고 prediction 결과 행렬을 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1080, 1920, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#테스트할 이미지를 행렬로 로드\n",
    "src = cv2.imread(\"test_images/test_image.png\")\n",
    "src_converted = cv2.cvtColor(src, cv2.COLOR_BGR2RGB)\n",
    "src_converted.shape # -> (2268, 4032, 3) 4032 x 2268 사이즈의 3채널 컬러이미지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 640, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#우리가 만든 모델은 (1, 640, 640, 3)을 입력으로 받으므로, 그에 맞게 resize & reshape을 진행\n",
    "#입력에 차원이 하나 더 붙어있는 이유는 배치 때문으로, resize 후 reshape을 통해 변환\n",
    "src_resized = cv2.resize(src_converted, (640, 640))\n",
    "src_resized.shape # -> (640, 640, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 640, 640, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_resized = src_resized.reshape((1, 640, 640, 3))\n",
    "src_resized.shape # -> (1, 640, 640, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1, 25200, 7), dtype=float32, numpy=\n",
       " array([[[7.9063047e-03, 5.0242371e-03, 1.7675599e-02, ...,\n",
       "          6.9760204e-06, 4.8929006e-01, 5.2223915e-01],\n",
       "         [7.4648722e-03, 4.6702134e-03, 1.4114226e-02, ...,\n",
       "          6.0116304e-06, 4.2623749e-01, 5.5148995e-01],\n",
       "         [8.3673317e-03, 5.0265356e-03, 2.1602832e-02, ...,\n",
       "          4.5579218e-06, 5.6351501e-01, 3.9660180e-01],\n",
       "         ...,\n",
       "         [9.8988801e-01, 9.8077792e-01, 1.3276212e-02, ...,\n",
       "          6.9653647e-06, 2.8037885e-01, 6.4101899e-01],\n",
       "         [9.9130201e-01, 9.8380512e-01, 4.2167697e-02, ...,\n",
       "          2.3403245e-06, 2.4748440e-01, 6.6107756e-01],\n",
       "         [9.8839682e-01, 9.8233902e-01, 6.5405197e-02, ...,\n",
       "          1.1161677e-06, 1.7683490e-01, 6.7621481e-01]]], dtype=float32)>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#추론 실행\n",
    "src_resized = src_resized/255. #픽셀값이 0~1 사이 값으로 rescaling되어 트레이닝 되었음. 추론도 0~1 사이의 값으로 변환하고 시도해야 함\n",
    "pred = model(src_resized)\n",
    "pred # -> 리스트로 추론 결과가 묶여서 출력. 따라서 실제 추론결과는 pred[0]으로 접근해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 25200, 7])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0].shape # -> TensorShape([1, 25200, 7])  *numpy가 아니라 tf 프레임웍 포맷에 맞춰 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25200, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "결과물 TensorShape([1, 25200, 7])의 제일 첫 차원은 의미 없고 나머지 차원이 중요 -> 25200행 * 7열로 이해할 것\n",
    "25200행 : 입력에 따라 추론한 25200개의 객체검출 결과\n",
    "7열 : 행별 결과내용 → [cx, cy, w, h, obj conf, c1 conf, c2 conf]\n",
    "  - 0~3번째 열은 검출한 바운딩 박스 정보\n",
    "  - 4번째 열은 바운딩 박스 안에 객체가 있을 확률\n",
    "  - 5번째 열은 검출한 객체가 1번 클래스(도로표지판)일 확률\n",
    "  - 6번째 열은 검출한 객체가 2번 클래스(신호등)일 확률\n",
    "\"\"\"\n",
    "\n",
    "pred = pred[0][0].numpy() #작업 용이성을 위해 필요없는 첫 차원은 제거하고 (25200, 7)만 선택, numpy 행렬로 변환한 후 pred에 덮어 씌움\n",
    "pred.shape # -> (25200, 7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 추론한 바운딩 박스 정보를 원본 이미지에 그린 후 결과를 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#25200개를 모두 화면에 그리면 너무 복잡함. obj conf. 순으로 정렬한 후 상위 3000개만 뽑아서 출력해 본다.\n",
    "#pred의 4번째 열(-> obj conf.)을 기준으로 내림차순 정렬\n",
    "rank = np.argsort(-pred[:, 4]) # -> array([24119, 24116, 24113, ...,  2613,  1893,  1653], dtype=int64)\n",
    "pred_sorted = pred[rank]\n",
    "\n",
    "dst = cv2.resize(src, (int((900*src.shape[1])/src.shape[0]), 900), interpolation=cv2.INTER_LINEAR) #이미지가 너무 크므로 리사이즈한다.\n",
    "for bbox in pred_sorted[:3000]:\n",
    "    cx = int(bbox[0] * dst.shape[1])\n",
    "    cy = int(bbox[1] * dst.shape[0])\n",
    "    w = int(bbox[2] * dst.shape[1])\n",
    "    h = int(bbox[3] * dst.shape[0])\n",
    "    x = int(cx - (w/2))\n",
    "    y = int(cy - (h/2))\n",
    "\n",
    "    dst = cv2.rectangle(dst, (x, y, w, h), (0, 255, 0), 2)\n",
    "cv2.imshow(\"dst\", dst)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과를 보면 오브젝트가 있는 곳 주변으로 바운딩박스가 많이 겹쳐져 나온다는 것을 알 수 있습니다. 겹쳐져 있는 바운딩박스를 처리하기 위한 Non Maximun Suppresion 작업이 필요하다는 것을 알 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('venv_common': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00788648533811c0d4af57f0819dead35b430506c7c842538b9d24915bb6fd49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
