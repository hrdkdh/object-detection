{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch 모델 파일을 tensorflow 파일로 변환"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드를 실행시켜 학습이 완료된 pytorch 모델파일(best.pt)을 tensorflow 모델파일(saved model)로 변환합니다.\n",
    "\n",
    ">{가상환경 경로명}\\Scripts\\python.exe {yolov5-master 경로명}\\export.py --weights {yolov5-master 경로명}\\runs\\train\\exp\\weights\\best.pt --include saved_model\n",
    "\n",
    "{yolov5-master 경로명}\\runs\\train\\exp\\weights 폴더로 들어가면 tensorflow 모델로 변환된 'best_saved_model' 폴더가 생성된 것을 확인할 수 있습니다.\n",
    "\n",
    "'best_saved_model' 폴더를 실습파일이 있는 경로(현재 보고 있는 이 주피터 노트북 파일이 있는 경로)로 복사해 옵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트 이미지를 입력으로 넣어 prediction하고 바운딩박스를 그려 이미지로 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. tf로 변환된 모델(pb)을 로드해 모델 객체를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf saved_model로 변환된 모델 파일 로드\n",
    "model = tf.saved_model.load(\"../best_saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델의 입출력 구조 확인 → 입력은 (1, 640, 640, 3), 출력은 (1, 25200, 7)임을 알 수 있음\n",
    "for k, v in model.signatures.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 이미지 하나를 로드해 모델에 입력으로 넣고 prediction 결과 행렬을 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = cv2.imread(\"../test_images/test_image.png\") #테스트할 이미지를 opencv를 이용해 행렬로 로드\n",
    "src_converted = cv2.cvtColor(src, cv2.COLOR_BGR2RGB) #우리가 만든 모델은 RGB 기준으로 트레이닝되었으므로 그에 맞춰 색공간을 변경\n",
    "src_converted.shape # -> (1080, 1920, 3) 1920 x 1080 사이즈의 3채널 컬러이미지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#우리가 만든 모델은 (1, 640, 640, 3)을 입력으로 받으므로, 그에 맞게 resize & reshape을 진행\n",
    "#입력에 차원이 하나 더 붙어있는 이유는 배치 때문으로, resize 후 reshape을 통해 변환\n",
    "src_resized = cv2.resize(src_converted, (640, 640))\n",
    "src_resized.shape # -> (640, 640, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_resized = src_resized.reshape((1, 640, 640, 3))\n",
    "src_resized.shape # -> (1, 640, 640, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#추론 실행\n",
    "src_resized = src_resized/255. #픽셀값이 0~1 사이 값으로 rescaling되어 트레이닝 되었음. 추론도 0~1 사이의 값으로 변환하고 시도해야 함\n",
    "pred = model(src_resized)\n",
    "pred # -> 리스트로 추론 결과가 묶여서 출력. 따라서 실제 추론결과는 pred[0]으로 접근해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[0].shape # -> TensorShape([1, 25200, 7])  *numpy가 아니라 tf 프레임웍 포맷에 맞춰 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "결과물 TensorShape([1, 25200, 7])의 제일 첫 차원은 의미 없고 나머지 차원이 중요 -> 25200행 * 7열로 이해할 것\n",
    "25200행 : 입력에 따라 추론한 25200개의 객체검출 결과\n",
    "7열 : 행별 결과내용 → [cx, cy, w, h, obj conf, c1 conf, c2 conf]\n",
    "  - 0~3번째 열은 검출한 바운딩 박스 정보\n",
    "  - 4번째 열은 바운딩 박스 안에 객체가 있을 확률\n",
    "  - 5번째 열은 검출한 객체가 1번 클래스(도로표지판)일 확률\n",
    "  - 6번째 열은 검출한 객체가 2번 클래스(신호등)일 확률\n",
    "\"\"\"\n",
    "\n",
    "pred = pred[0][0].numpy() #작업 용이성을 위해 필요없는 첫 차원은 제거하고 (25200, 7)만 선택, numpy 행렬로 변환한 후 pred에 덮어 씌움\n",
    "pred.shape # -> (25200, 7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 추론한 바운딩 박스 정보를 원본 이미지에 그린 후 결과를 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#25200개를 모두 화면에 그리면 너무 복잡함. obj conf. 순으로 정렬한 후 상위 3000개만 뽑아서 출력\n",
    "#pred의 4번째 열(-> obj conf.)을 기준으로 내림차순 정렬\n",
    "rank = np.argsort(-pred[:, 4]) # -> array([24119, 24116, 24113, ...,  2613,  1893,  1653], dtype=int64)\n",
    "pred_sorted = pred[rank]\n",
    "\n",
    "dst = cv2.resize(src, (int((900*src.shape[1])/src.shape[0]), 900), interpolation=cv2.INTER_LINEAR) #이미지가 너무 크므로 리사이즈\n",
    "for bbox in pred_sorted[:3000]: #25200개를 모두 그리면 너무 많으므로 3000개만 그려봄\n",
    "    cx = int(bbox[0] * dst.shape[1])\n",
    "    cy = int(bbox[1] * dst.shape[0])\n",
    "    w = int(bbox[2] * dst.shape[1])\n",
    "    h = int(bbox[3] * dst.shape[0])\n",
    "    x = int(cx - (w/2))\n",
    "    y = int(cy - (h/2))\n",
    "\n",
    "    dst = cv2.rectangle(dst, (x, y, w, h), (0, 255, 0), 2)\n",
    "cv2.imshow(\"dst\", dst)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과를 보면 오브젝트가 있는 곳 주변으로 바운딩박스가 많이 겹쳐져 나온다는 것을 알 수 있습니다. 겹쳐져 있는 바운딩박스를 정리하기 위한 추가 작업(Non Maximun Suppresion)이 필요하다는 것을 알 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('venv_common': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00788648533811c0d4af57f0819dead35b430506c7c842538b9d24915bb6fd49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
